"""
Module de mod√©lisation avec ensemble learning pour ExoKeplerAI
Combine LightGBM, CatBoost et XGBoost
"""

import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix
import lightgbm as lgb
import catboost as cb
import xgboost as xgb
import joblib
import warnings
warnings.filterwarnings('ignore')


class ExoplanetEnsembleClassifier:
    """
    Classificateur d'ensemble combinant LightGBM, CatBoost et XGBoost
    """
    
    def __init__(self, random_state=42):
        """
        Initialise les trois mod√®les
        
        Args:
            random_state: Seed pour la reproductibilit√©
        """
        self.random_state = random_state
        self.models = {}
        self.weights = {'lgb': 0.33, 'cat': 0.34, 'xgb': 0.33}  # Poids initiaux √©gaux
        self.class_names = ['FALSE POSITIVE', 'CANDIDATE', 'CONFIRMED']
        
        # Initialiser les mod√®les
        self._initialize_models()
    
    def _initialize_models(self):
        """
        Initialise les trois mod√®les avec leurs hyperparam√®tres
        """
        print("üîß Initialisation des mod√®les...")
        
        # LightGBM
        self.models['lgb'] = lgb.LGBMClassifier(
            n_estimators=500,
            learning_rate=0.05,
            max_depth=8,
            num_leaves=31,
            min_child_samples=20,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=self.random_state,
            verbose=-1,
            class_weight='balanced'
        )
        
        # CatBoost
        self.models['cat'] = cb.CatBoostClassifier(
            iterations=500,
            learning_rate=0.05,
            depth=8,
            l2_leaf_reg=3,
            random_state=self.random_state,
            verbose=False,
            auto_class_weights='Balanced'
        )
        
        # XGBoost
        self.models['xgb'] = xgb.XGBClassifier(
            n_estimators=500,
            learning_rate=0.05,
            max_depth=8,
            min_child_weight=1,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=self.random_state,
            verbosity=0,
            eval_metric='mlogloss'
        )
        
        print("   LightGBM initialis√©")
        print("   CatBoost initialis√©")
        print("   XGBoost initialis√©")
    
    def train(self, X_train, y_train, X_val=None, y_val=None):
        """
        Entra√Æne les trois mod√®les
        
        Args:
            X_train: Features d'entra√Ænement
            y_train: Target d'entra√Ænement
            X_val: Features de validation (optionnel)
            y_val: Target de validation (optionnel)
        """
        print("ENTRA√éNEMENT DES MOD√àLES")
        
        # Entra√Æner LightGBM
        print("Entra√Ænement de LightGBM...")
        if X_val is not None and y_val is not None:
            self.models['lgb'].fit(
                X_train, y_train,
                eval_set=[(X_val, y_val)],
                eval_metric='multi_logloss',
                callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)]
            )
        else:
            self.models['lgb'].fit(X_train, y_train)
        print("   LightGBM entra√Æn√©")
        
        # Entra√Æner CatBoost
        print("\nEntra√Ænement de CatBoost...")
        if X_val is not None and y_val is not None:
            self.models['cat'].fit(
                X_train, y_train,
                eval_set=(X_val, y_val),
                early_stopping_rounds=50,
                verbose=False
            )
        else:
            self.models['cat'].fit(X_train, y_train)
        print("   CatBoost entra√Æn√©")
        
        # Entra√Æner XGBoost
        print("\nEntra√Ænement de XGBoost...")
        if X_val is not None and y_val is not None:
            self.models['xgb'].fit(
                X_train, y_train,
                eval_set=[(X_val, y_val)],
                verbose=False
            )
        else:
            self.models['xgb'].fit(X_train, y_train)
        print("   XGBoost entra√Æn√©")
        
        print(" ENTRA√éNEMENT TERMIN√â")
    
    def predict_proba(self, X):
        """
        Pr√©dit les probabilit√©s avec ensemble voting
        
        Args:
            X: Features
            
        Returns:
            Probabilit√©s pour chaque classe
        """
        # Obtenir les probabilit√©s de chaque mod√®le
        proba_lgb = self.models['lgb'].predict_proba(X)
        proba_cat = self.models['cat'].predict_proba(X)
        proba_xgb = self.models['xgb'].predict_proba(X)
        
        # Moyenne pond√©r√©e
        proba_ensemble = (
            self.weights['lgb'] * proba_lgb +
            self.weights['cat'] * proba_cat +
            self.weights['xgb'] * proba_xgb
        )
        
        return proba_ensemble
    
    def predict(self, X):
        """
        Pr√©dit les classes avec ensemble voting
        
        Args:
            X: Features
            
        Returns:
            Pr√©dictions de classe
        """
        proba = self.predict_proba(X)
        return np.argmax(proba, axis=1)
    
    def evaluate(self, X_test, y_test, detailed=True):
        """
        √âvalue les performances du mod√®le
        
        Args:
            X_test: Features de test
            y_test: Target de test
            detailed: Afficher les d√©tails
            
        Returns:
            Dictionnaire avec les m√©triques
        """
        print(" √âVALUATION DES PERFORMANCES")
        
        # Pr√©dictions de l'ensemble
        y_pred_ensemble = self.predict(X_test)
        
        # M√©triques globales
        metrics = {
            'accuracy': accuracy_score(y_test, y_pred_ensemble),
            'precision': precision_score(y_test, y_pred_ensemble, average='weighted'),
            'recall': recall_score(y_test, y_pred_ensemble, average='weighted'),
            'f1_score': f1_score(y_test, y_pred_ensemble, average='weighted')
        }
        
        if detailed:
            print(" MOD√àLE D'ENSEMBLE")
            print(f"   Accuracy:  {metrics['accuracy']:.4f}")
            print(f"   Precision: {metrics['precision']:.4f}")
            print(f"   Recall:    {metrics['recall']:.4f}")
            print(f"   F1-Score:  {metrics['f1_score']:.4f}")
            
            # √âvaluer chaque mod√®le individuellement
            print("\n PERFORMANCES INDIVIDUELLES")
            
            for name, model in self.models.items():
                y_pred = model.predict(X_test)
                acc = accuracy_score(y_test, y_pred)
                print(f"   {name.upper()}: Accuracy = {acc:.4f}")
            
            # Rapport de classification d√©taill√©
            print("\n RAPPORT DE CLASSIFICATION")
            print(classification_report(
                y_test, y_pred_ensemble,
                target_names=self.class_names,
                digits=4
            ))
            
            # Matrice de confusion
            print(" MATRICE DE CONFUSION")
            cm = confusion_matrix(y_test, y_pred_ensemble)
            print(pd.DataFrame(
                cm,
                index=[f'True {c}' for c in self.class_names],
                columns=[f'Pred {c}' for c in self.class_names]
            ))
        
        
        return metrics
    
    def get_feature_importance(self, feature_names, top_n=20):
        """
        Obtient l'importance des features de chaque mod√®le
        
        Args:
            feature_names: Noms des features
            top_n: Nombre de top features √† retourner
            
        Returns:
            DataFrame avec l'importance des features
        """
        # LightGBM
        importance_lgb = pd.DataFrame({
            'feature': feature_names,
            'importance_lgb': self.models['lgb'].feature_importances_
        })
        
        # CatBoost
        importance_cat = pd.DataFrame({
            'feature': feature_names,
            'importance_cat': self.models['cat'].feature_importances_
        })
        
        # XGBoost
        importance_xgb = pd.DataFrame({
            'feature': feature_names,
            'importance_xgb': self.models['xgb'].feature_importances_
        })
        
        # Fusionner
        importance = importance_lgb.merge(importance_cat, on='feature').merge(importance_xgb, on='feature')
        
        # Moyenne pond√©r√©e
        importance['importance_avg'] = (
            self.weights['lgb'] * importance['importance_lgb'] +
            self.weights['cat'] * importance['importance_cat'] +
            self.weights['xgb'] * importance['importance_xgb']
        )
        
        # Trier et retourner top N
        importance = importance.sort_values('importance_avg', ascending=False).head(top_n)
        
        return importance
    
    def save_models(self, directory='models'):
        """
        Sauvegarde les mod√®les entra√Æn√©s
        
        Args:
            directory: R√©pertoire de sauvegarde
        """
        import os
        os.makedirs(directory, exist_ok=True)
        
        print(f"\n Sauvegarde des mod√®les dans '{directory}/'...")
        
        # Sauvegarder chaque mod√®le
        joblib.dump(self.models['lgb'], f'{directory}/lightgbm_model.pkl')
        self.models['cat'].save_model(f'{directory}/catboost_model.cbm')
        self.models['xgb'].save_model(f'{directory}/xgboost_model.json')
        
        # Sauvegarder les poids
        joblib.dump(self.weights, f'{directory}/ensemble_weights.pkl')
        
        print("    LightGBM sauvegard√©")
        print("    CatBoost sauvegard√©")
        print("    XGBoost sauvegard√©")
        print("    Poids d'ensemble sauvegard√©s")
    
    def load_models(self, directory='models'):
        """
        Charge les mod√®les sauvegard√©s
        
        Args:
            directory: R√©pertoire de chargement
        """
        print(f"\n Chargement des mod√®les depuis '{directory}/'...")
        
        # Charger chaque mod√®le
        self.models['lgb'] = joblib.load(f'{directory}/lightgbm_model.pkl')
        self.models['cat'] = cb.CatBoostClassifier()
        self.models['cat'].load_model(f'{directory}/catboost_model.cbm')
        self.models['xgb'] = xgb.XGBClassifier()
        self.models['xgb'].load_model(f'{directory}/xgboost_model.json')
        
        # Charger les poids
        self.weights = joblib.load(f'{directory}/ensemble_weights.pkl')
        
        print("    LightGBM charg√©")
        print("    CatBoost charg√©")
        print("    XGBoost charg√©")
        print("    Poids d'ensemble charg√©s")
    
    def predict_single(self, X):
        """
        Pr√©dit pour une seule observation avec d√©tails
        
        Args:
            X: Features (1 observation)
            
        Returns:
            Dictionnaire avec pr√©diction et probabilit√©s
        """
        proba = self.predict_proba(X)[0]
        pred_class = np.argmax(proba)
        
        # Obtenir les pr√©dictions individuelles avec conversion s√©curis√©e
        lgb_pred = int(self.models['lgb'].predict(X)[0])
        cat_pred = int(self.models['cat'].predict(X)[0])
        xgb_pred = int(self.models['xgb'].predict(X)[0])
        
        return {
            'prediction': self.class_names[pred_class],
            'prediction_code': int(pred_class),
            'confidence': float(proba[pred_class]),
            'probabilities': {
                'FALSE POSITIVE': float(proba[0]),
                'CANDIDATE': float(proba[1]),
                'CONFIRMED': float(proba[2])
            },
            'individual_predictions': {
                'LightGBM': self.class_names[lgb_pred],
                'CatBoost': self.class_names[cat_pred],
                'XGBoost': self.class_names[xgb_pred]
            }
        }
